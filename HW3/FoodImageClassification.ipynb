{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b149a774-d2e1-4274-9842-53ac22067892",
   "metadata": {},
   "source": [
    "# 前置知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e3201-2062-43dd-a7a3-29d7f56b0150",
   "metadata": {},
   "source": [
    "## k-foldCrossValidation Pattern\n",
    "[F-fold 官方文档](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4e57a55-b030-4e11-af16-a28790e7aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "KFold(n_splits=3, random_state=42, shuffle=True)\n",
      "Fold 0:\n",
      "  Train: index=[2 3 4 6 7]\n",
      "  val:  index=[0 1 5]\n",
      "Fold 1:\n",
      "  Train: index=[0 1 3 5 6]\n",
      "  val:  index=[2 4 7]\n",
      "Fold 2:\n",
      "  Train: index=[0 1 2 4 5 7]\n",
      "  val:  index=[3 6]\n"
     ]
    }
   ],
   "source": [
    "#使用\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "print(kf.get_n_splits(X))\n",
    "print(kf)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  val:  index={val_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9454f-1b94-4304-9a99-11508ded7dd2",
   "metadata": {},
   "source": [
    "# HW3\n",
    "HW3要解决的问题是实物图像分类，这次HW比较有挑战性，如果要做到StrongBaseline或是BossBaseline,都需要用到  \n",
    "许多没有了解的策略。比如Augmentation以及Testing，或是Training的技巧，测试上也可以用到集成的voting。以及  \n",
    "将Augmentation用于Test Time Augmentation以及多模型预测结果的加权来保证测试结果的稳定性。设计上也要参考一   \n",
    "些经典论文上提到的模型比如ResidualNet等  \n",
    "\n",
    "先列一下大纲吧，要实现的东西有点多。  \n",
    "\n",
    "1.数据处理\n",
    "* `训练数据预处理`  由于网络结构中要用到CNN，图像根据固定输入大小如何预处理  \n",
    "* `k-fold split`  将原来已近划分好的训练集验证集进行划分  \n",
    "* `DataAugmentation`  由于模型可能构建的比较复杂，所以必须数据增强以避免过拟合。以及将数据增强方法用到测试集上\n",
    "\n",
    "2.模型实现  （PreTrained Model NOT ALLOWED）\n",
    "* `ResidualNet` 残差网络的使用  \n",
    "* `CNN以及polling` CNN以及polling的使用(downSampling words better)  \n",
    "* `SpatialTransform` ST层需要自己实现吗？  \n",
    "* `ensemble` 实现并集成多模型的voting结果  \n",
    "* `ClassificationTricks` Label smoothing Cross Entropy Loss、FocalLoss  \n",
    "* `OptimTricks` Dropout、BatchNorm、GradientAccumulation、ImageNormalization\n",
    "\n",
    "3.Training/Validation/Testing\n",
    "* `Training` k-fold交叉验证实现思路：在最后训练时实现，定义kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "枚举kfold.split(dataset)，之后利用torch.utils.data.SubsetRandomSampler初始化DataLoader就可以实现k-fold了\n",
    "* `Validation`\n",
    "* `Testing`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eac7c518-5492-4667-9cfb-98acc436d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader,ConcatDataset,Subset\n",
    "from torchvision.datasets import DatasetFolder,VisionDataset\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "\n",
    "#dataPreprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "832e3b09-2179-4099-82b1-550106e57c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#图像变换的随机性主要由python内置的random模块决定，所以设定随即种子是有必要的\n",
    "myseed = 420613\n",
    "random.seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "torch.backends.cudnn.derministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d52479-3f12-42b4-bfa6-07b0af62ebeb",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "679fbc0c-56d7-4c1c-a2c4-c22626ca58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa60a11-d90e-4b9c-8195-0080ae5c441a",
   "metadata": {},
   "source": [
    "## Transfroms/Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31e6001f-e2ae-4801-805c-29f781eebe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "训练集转换层，这些变换太多了，具体的变换见torch官网文档中的Examples and training references\n",
    "并且对于RandomChoice，Compose，RandomApply第一个参数都是可执行变换的参数列表，这些函数都是返回\n",
    "可执行变换的流水线列表，所以他们之间可以进行嵌套并自定义转换模型对数据进行处理\n",
    "'''\n",
    "\n",
    "train_tfm = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((128,128)),\n",
    "    #随机选择一个变换将其作用于图像，第二个参数p可以指定一个列表对应序列中变换被选择出的概率(自动标准化)\n",
    "    v2.RandomApply([\n",
    "        v2.RandomChoice([\n",
    "            v2.ColorJitter(brightness=0.4, contrast=0.2, hue =0.3), #随机改变图像的亮度、对比度、饱和度和hue\n",
    "            v2.RandomRotation(degrees=(0, 30)),                     #随机对图像在给定范围内正负rotation\n",
    "            v2.RandomInvert(p=1.0),                                 #按指定概率对图像颜色进行反转\n",
    "            v2.RandomSolarize(192, p=1.0),                          #按阈值对图像颜色进行反转\n",
    "            v2.RandomAutocontrast(p=1.0),                           #调整对比度\n",
    "            v2.RandomAdjustSharpness(2.0, p=1.0),                   #锐化\n",
    "            v2.RandomHorizontalFlip(p=1.0),                         #水平翻转\n",
    "        ])\n",
    "    ], p=0.35),\n",
    "    v2.ToDtype(torch.float32, scale=True)                           #scale是否将其值缩放到[0,1]之间即标准化\n",
    "])\n",
    "\n",
    "#通常我们不需要任何在val阶段和test阶段的augmentation，只需要resize再转化为tensor即可\n",
    "#但我们可以使用train_tfm来处理测试集来产生不同类型的图片，但使用集成方法来决定test的结果\n",
    "test_tfm = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(128, 128),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c27cb-5ff3-4bea-ba5d-cd0029cc2965",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "对于数据在模型输入之前的处理，有两种结构，第一种是将数据处理放在构造函数中，这样再getitem中便不用再做处理  \n",
    "第二种则是定义好处理的流水线，例如transfrom层，在getitem中使用数据时再将数据进行变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86c418cd-06fb-4e21-bbc9-5f06120ee0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, mode, path, tfm=test_tfm, files=None):\n",
    "        self.super(ImageDataset).__init__()\n",
    "        self.transform = tfm\n",
    "        \n",
    "        if mode == 'train' or mode == 'val':\n",
    "            '''\n",
    "            files:所有文件名的列表\n",
    "            train_path:原先划分好的训练数据path，待融合\n",
    "            val_path:原先划分好的val数据path，待融合\n",
    "            分别提取出train文件夹中的数据和validation文件夹中的文件名list，并merge为file便于交叉验证和其他处理\n",
    "            由于我们既然将两个文件夹中的图片融合了，那么他们之间就是等价的。无论怎么拼接都是可行的\n",
    "            '''\n",
    "            train_path, val_path = os.path.join(path,'training'), os.path.join(path,'validation')\n",
    "            files_test = sorted([os.path.join(train_path, x) for x in os.listdir(train_path) if x.endswith(\".jpg\")])\n",
    "            files_val = sorted([os.path.join(val_path, x) for x in os.listdir(val_path) if x.endswith(\".jpg\")])\n",
    "            self.files = files_test + files_val\n",
    "        else:\n",
    "            test_path = os.path.join(path,'test')\n",
    "            self.files = sorted([os.path.join(test_path, x) for x in os.listdir(test_path) if x.endswith('.jpg')])\n",
    "        if files != None :\n",
    "                self.files = files\n",
    "        print(f\"first of Image samples\",self.files[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        除了用transfrom对图像进行处理之外，还需要提取图像名中的标签。为文件名的开头第一个数字\n",
    "        结构 Folder1/Floder2/0_1234.jpg 在这个路径中生成以 / 为分隔符的列表，再取列表最后一个\n",
    "        元素，即'0_1234.jpg'进行处理,同理再进行一次 _ 分割就可以得到标签。\n",
    "\n",
    "        即使测试集用不到标签，我们也对其处理以下\n",
    "        '''\n",
    "        fname = self.files[index]\n",
    "        image = Image.open(fname)\n",
    "        image = self.transfrom(image)\n",
    "        if mode == 'test':\n",
    "            label = -1\n",
    "        else:    \n",
    "            try:\n",
    "                label = int(fname.split('/')[-1].split('_')[0])\n",
    "            except:\n",
    "                label = -1\n",
    "        return image, label\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e08342-8e81-4399-b382-c0ec474f5a83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model\n",
    "由于计算量太大，所以要求实现的网络比较浅而且并不是每一个连接处都有shortcut，而是隔一个才有一个\n",
    "\n",
    "由于resnet论文中并没有详细说明实现，只能看源码来学习数据流动了。经过查看源码对于原文中较浅的resnet来说\n",
    "未使用bottleNeck结构的结构如下：\n",
    "* 每次跳跃两层，并且identity与Residual总是在进入卷积层之前分离，在经过两层卷积层之后ReLU之前汇合，对于主路线有如下数据流动方式\n",
    "* Residual与identity分离 Residual进入->Conv->BatchNorm->ReLU->Conv->Batch-> Residual + identity ->ReLU\n",
    "从下方的源码和原论文中的模型结构可以看出只有中间四个模块化主要层是进行了分离处理的，开头7 * 7的卷积核处理、batchNorm\n",
    "ReLU和pool都是将残差和identity并不分离一起处理的，这一点在代码实现中需要注意。在经过四个主要卷积模块处理之后\n",
    "最后也是将数据都汇入主干进行进行平均池化和全连接层的处理，全连接层设置在softmax之前是必要的\n",
    "\n",
    "\n",
    "```python\n",
    "    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding\n",
    "        =1)\n",
    "        # 通过_make_layer带到层次化设，只有中间这四行中包含shortcut实现计的效果\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])  # 对应着conv2_x\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])  # 对应着conv3_x\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])  # 对应着conv4_x\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])  # 对应着conv5_x\n",
    "        # 分类头\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expan\n",
    "### 步幅为1的卷积和步幅为2的最大池化还是直接步幅为2的下采样？\n",
    "* 卷积 + 步长为2最大池化：\n",
    "在很多网络架构中，如 VGG，卷积层后跟最大池化是一种常见的设计。它可以在保持特征表达能力的同时减少计算量。\n",
    "这种设计可以使网络更具稳定性，因为最大池化减少了特征图的空间尺寸，并且有助于防止过拟合。\n",
    "\n",
    "* 步长为 2 的卷积(SownSampling)：\n",
    "在许多现代卷积神经网络（如 ResNet 和 DenseNet）中，步长为 2 的卷积是常见的下采样方法。这种方法可以更有效地融合特征，因为它在下采样的同时进行特征提取。\n",
    "使用步长为 2 的卷积可以提高网络的表达能力，因为卷积操作能够整合更多的信息，从而可以表现得更好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec0bf4-2dcf-482b-9f56-1813ba0e3da0",
   "metadata": {},
   "source": [
    "<img src = 'structure.png' alt = 'structure.png' width = '600' position = 'center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4fe5f67d-83bd-47f7-842e-4202b3954768",
   "metadata": {},
   "outputs": [],
   "source": [
    " # torch.nn.Conv2d(in_channels, out_channels, kernel_size(square), stride, padding)\n",
    "# torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "    '''\n",
    "    基础模块以及identity shortcut的实现\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 inplanes: int,\n",
    "                 midplanes: int,\n",
    "                 outplanes: int):\n",
    "        \n",
    "        self.super(BasicBlock, self).__init__()\n",
    "        \n",
    "        #self.cnn = nn.Sequential(\n",
    "        #由于在实现Dataset的数据增强或是只是做transfrom处理的部分已经进行了标准化，所以在模型的初始阶段无需进行标准化\n",
    "        self.conv1 = nn.Conv2d(inplanes, midplanes, kernel_size=3, stirde=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(midplanes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)#can optionally do the operation in-place. Default: False\n",
    "        # self.pool1 = nn.MaxPool2d(3, 2, 0) 抛弃最大池化而采用下采样来降低维度\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(midplanes, outplanes, kernel_size=3, stride=2, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(outplanes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # self.pool2 = nn.MaxPool2d(3, 2, 0)\n",
    "\n",
    "        #下采样 用于处理shortcut传递identity与主线路大小不匹配的问题\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv(inplanes, midplanes, kernel_size=1, stride=2),\n",
    "            # nn.Conv(midplanes, outplanes, kernel_size=1, stride=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        identity = x                             #残差和恒等映射分离\n",
    "        residual = self.conv1(residual)\n",
    "        residual = self.bn1(residual)\n",
    "        identity = self.downSamples(identity)    #下采样以使得identity特征图的长宽和主线路上的长宽相同\n",
    "        x = residual + identity                  #根据要求给出的网络结构残差只跳过一层卷积层就回到主线路\n",
    "        x = self.ReLU(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.ReLU(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c494f2cc-eaf8-427a-9ce0-6916bbcab73a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # torch.nn.Conv2d(in_channels, out_channels, kernel_size(square), stride, padding)\n",
    "# torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "class Classifier(nn.Module):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.super(Classifier, self).__init__()\n",
    "        #做类似ResidualNet原论文中初始化时候的一个处理，最然我也不知道为什么，今天就这样吧 23：56/8-1\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=5, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.cnn = nn.Sequential(\n",
    "            BasicBlock(32, 32, 64),\n",
    "            BasicBlock(64, 64, 128),\n",
    "            BasicBlock(128, 256, 256),\n",
    "            BasicBlock(256, 512, 512),\n",
    "        )\n",
    "        #平均池化 + \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),    #这一步很重要平均池化之后经过全连接层进行处理\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11),\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.cnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def cal_loss(self, preds, labels):\n",
    "        return self.criterion(preds, labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed64fa-acf5-487a-936d-fd9dcf6dc299",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train function\n",
    "* 对张量除了做类型变换之外的操作得到的也是张量，所以每次需要在最后进行转换.item() .numpy()转化为数值类型或是数组类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1b2feea-38eb-408e-8177-eaacb1318c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "def train(train_set: DataLoader, \n",
    "          val_set: DataLoader, \n",
    "          model: nn.Module, \n",
    "          config: Any, \n",
    "          device: torch.device) -> None:\n",
    "    '''\n",
    "        通常train都需要五个参数以上\n",
    "        train_set:training dataset(dataLoader)\n",
    "        val_set:validation dataset(dataLoader)\n",
    "        model:model instance\n",
    "        config:hyperParameter\n",
    "        device:compute device\n",
    "    '''\n",
    "    #-----------------------------training--------------------------------#\n",
    "    model.train()\n",
    "    optimizer = getattr(torch.optim,config['optimizer'])(\n",
    "        model.parameters(),**confit['optimizer_hParas'])\n",
    "    n_epochs = config['n_epochs']\n",
    "    epoch = 0\n",
    "    best_acc = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        train_batch_loss = []\n",
    "        train_batch_accs = []\n",
    "        for x, y in train_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = model.cal_loss(preds, y)\n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm = 120)\n",
    "            optimizer.step()\n",
    "\n",
    "            #Another method to calculate batchAcc,下面这种计算一个batch的acc的方式与HW2中的计算Acc的方法显然更好\n",
    "            #比较简洁并且不容易出错，不需要考虑len(x)而是指即取平均值\n",
    "            acc = (preds.argmax(dim=-1).detach() == y.detach()).float().mean()\n",
    "            train_batch_accs.append(acc)\n",
    "            \n",
    "            loss_detached = loss.detach().cpu().item()      #detach loss\n",
    "            train_batch_loss.append(loss_detached)\n",
    "            \n",
    "        train_loss = sum(train_batch_loss) / len(train_batch_loss)\n",
    "        train_acc = sum(train_batch_accs) / len(train_batch_accs)\n",
    "        print('[{:03d}/{:03d}] Train Acc{:3.6f} Train Loss{:3.6f}'.format(epoch+1, n_epochs, train_acc, train_loss))\n",
    "        \n",
    "        #-----------------------------validation--------------------------------#\n",
    "        if len(val_set) > 0:\n",
    "            val_acc, val_loss = validate(val_set, model, device)\n",
    "            print('Val Acc{:3.6f} Val Acc{:3.6f}'.format(val_acc, val_loss))\n",
    "    \n",
    "            if val_acc > best_acc :\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(),config['save_path'])\n",
    "                print('Saving model at opoch {:03d} Better Acc {:3.3f}'.format(epoch + 1, val_acc))\n",
    "            \n",
    "            else:\n",
    "                print('Not a better Acc {:3.3f}'.format(epoch + 1,val_acc))\n",
    "        elif len(val_set) == 0:\n",
    "            torch.save(model.state_dict(), config['save_path'])\n",
    "            print('Saving model at last epoch')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444e260-22dc-4bea-8bbd-a69799f33509",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1af8bb6a-9982-4ba3-aa42-855e9e6e7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_set: DataLoader, model:nn.Module, device:torch.device):\n",
    "    model.eval()\n",
    "    val_epoch_acc = []\n",
    "    val_epoch_loss = []\n",
    "    for x, y in val_set:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(x)\n",
    "            loss = model.cal_loss(preds, y).detach().cpu().item()\n",
    "        acc = (preds.argmax(dim=-1).detach() == y.detach()).float().mean()\n",
    "        val_epoch_acc.append(acc)\n",
    "        val_epoch_loss.append(loss)\n",
    "    return sum(val_epoch_acc) / len(val_epoch_acc), sum(val_epoch_loss) / len(val_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851492bb-5531-4cd0-8a59-c4f67815f91f",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "112bf2f6-add9-4c86-9cea-e9380d8774ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_set: DataLoader, model: nn.Module, device: torch.device):\n",
    "    '''\n",
    "        测试函数，使用训练好的模型对需要预测的数据进行预测\n",
    "        这里的test实现需要用到数据增强实现\n",
    "    '''\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for x, y in test_set:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            _,pred_class = torch.max(preds,dim=-1)#max函数返回的是两个张量，在这个问题中返回的是两个一维张量\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim = 0).numpy()\n",
    "    return preds\n",
    "\n",
    "def save_preds(preds, file):\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w') as fp:\n",
    "        writer.writerow(['Id', 'Class'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab9844-7842-4bf6-bdcb-cc7a3da2a862",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3757037-305f-42ea-9c83-c77bc3f95207",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "config = {\n",
    "    'n_epochs':10,\n",
    "    'batch_size':512,\n",
    "    'optimizer':'Adam',\n",
    "    'optimizer_hParas':{\n",
    "        'lr':1e-3,\n",
    "        'monmentum':0.9,\n",
    "        'weight_decay':5e-4\n",
    "    },\n",
    "    'save_path':'models/model.pth'\n",
    "}\n",
    "path = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d9b27-ddab-450e-b688-7e5a1a956075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于要使用K-fold所以将\n",
    "tr_dataset = ImageDataset('train', path=path, tfm=train_tfm)\n",
    "test_dataset = ImageDataset('test', path=path, tfm=test_tfm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

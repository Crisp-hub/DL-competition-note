{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b149a774-d2e1-4274-9842-53ac22067892",
   "metadata": {},
   "source": [
    "# 前置知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b4dd61-2ce0-4cf0-9406-bd0ce4b015da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {font-size: initial;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Reset CSS\n",
    "display(HTML('<style>body {font-size: initial;}</style>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e3201-2062-43dd-a7a3-29d7f56b0150",
   "metadata": {},
   "source": [
    "## k-foldCrossValidation Pattern\n",
    "[F-fold 官方文档](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb4a58-b820-47f6-b88c-9d2e95863f6c",
   "metadata": {},
   "source": [
    "两种实现K-Fold交叉验证的方法,两者的区别在于是使用Sampler还是Subset\n",
    "\n",
    "### 方法一：使用Range以及SubsetRandomSampler\n",
    "具体实现就是TrainingTime中所做的实现\n",
    "\n",
    "### 方法二：使用Range以及Subset\n",
    "```python\n",
    "\n",
    "# 实例化dataset对象\n",
    "dataset = ImageDataset('train', path)\n",
    "\n",
    "# 初始化KFold对象\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 遍历每个fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(dataset)))):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    \n",
    "    # 通过索引创建训练和验证子集\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    \n",
    "    # 创建DataLoader对象\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5923a-005e-4d31-95d7-af10b0500c77",
   "metadata": {},
   "source": [
    "# HW3\n",
    "HW3要解决的问题是实物图像分类，这次HW比较有挑战性，如果要做到StrongBaseline或是BossBaseline,都需要用到  \n",
    "许多没有了解的策略。比如Augmentation以及Testing，或是Training的技巧，测试上也可以用到集成的voting。以及  \n",
    "将Augmentation用于Test Time Augmentation以及多模型预测结果的加权来保证测试结果的稳定性。设计上也要参考一   \n",
    "些经典论文上提到的模型比如ResidualNet等  \n",
    "\n",
    "先列一下大纲吧，要实现的东西有点多。  \n",
    "\n",
    "1.数据处理\n",
    "* `训练数据预处理`  由于网络结构中要用到CNN，图像根据固定输入大小如何预处理  \n",
    "* `k-fold split`  将原来已近划分好的训练集验证集进行划分  \n",
    "* `DataAugmentation`  由于模型可能构建的比较复杂，所以必须数据增强以避免过拟合。以及将数据增强方法用到测试集上\n",
    "\n",
    "2.模型实现  （PreTrained Model NOT ALLOWED）\n",
    "* `ResidualNet` 残差网络的使用  \n",
    "* `CNN以及polling` CNN以及polling的使用(downSampling words better)  \n",
    "* `SpatialTransform` ST层需要自己实现吗？  \n",
    "* `ensemble` 实现并集成多模型的voting结果  \n",
    "* `ClassificationTricks` Label smoothing Cross Entropy Loss、FocalLoss  \n",
    "* `OptimTricks` Dropout、BatchNorm、GradientAccumulation、ImageNormalization\n",
    "\n",
    "3.Training/Validation/Testing\n",
    "* `Training` k-fold交叉验证实现思路：在最后训练时实现，定义kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "枚举kfold.split(dataset)，之后利用torch.utils.data.SubsetRandomSampler初始化DataLoader就可以实现k-fold了\n",
    "* `Validation`\n",
    "* `Testing`\n",
    "\n",
    "coding需要注意的点：　　\n",
    "1.首先是编写网络架构经过每一层传递之后的张量大小有非常清楚的认识，主要是张量的　　　\n",
    "长Ｈ 和宽Ｗ　因为这两个因素在residualNet中在shortcut之后会有残差和恒等映射的　　\n",
    "相遇，这时候必须保证两个张量大小必须相同，但是习惯上对于identity映射中Ｈ和Ｗ不　　　\n",
    "不对等的处理要么是下采样，要么是maxpool。下采样kernelsize＝１而在主路径中，　　　\n",
    "kernelsize大小基本上不会是１这样就需要考虑padding大小的问题，如果选择了错误的　　　\n",
    "padding就会导致两个张量大小不匹配　　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac7c518-5492-4667-9cfb-98acc436d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader,ConcatDataset,Subset,SubsetRandomSampler\n",
    "from torchvision.datasets import DatasetFolder,VisionDataset\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "#dataPreprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832e3b09-2179-4099-82b1-550106e57c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#图像变换的随机性主要由python内置的random模块决定，所以设定随即种子是有必要的\n",
    "myseed = 420613\n",
    "random.seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "torch.backends.cudnn.derministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d52479-3f12-42b4-bfa6-07b0af62ebeb",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679fbc0c-56d7-4c1c-a2c4-c22626ca58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa60a11-d90e-4b9c-8195-0080ae5c441a",
   "metadata": {},
   "source": [
    "## Transforms/Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa695682-c13d-485b-a1ea-656389d82a6e",
   "metadata": {},
   "source": [
    "### 对于数据增强对模型训练效果的思考\n",
    "使用数据增强来训练模型，很容易让人产生下面思考，只考虑一个epoch：\n",
    "* 使用下面的tfm结构，一张图片在训练过程中(原图 | 增强图片)是不可能同时存在的\n",
    "* 对于不同的图片(whether same label or not)只使用一个版本是否会造成模型学习效果不佳\n",
    "* 采取其他措施：对于一张图片的原图/增强图片混合，但这样一定会增加数据集的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e6001f-e2ae-4801-805c-29f781eebe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "训练集转换层，这些变换太多了，具体的变换见torch官网文档中的Examples and training references\n",
    "并且对于RandomChoice，Compose，RandomApply第一个参数都是可执行变换的参数列表，这些函数都是返回\n",
    "可执行变换的流水线列表，所以他们之间可以进行嵌套并自定义转换模型对数据进行处理\n",
    "'''\n",
    "\n",
    "train_tfm = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((128,128)),\n",
    "    #随机选择一个变换将其作用于图像，第二个参数p可以指定一个列表对应序列中变换被选择出的概率(自动标准化)\n",
    "    v2.RandomApply([\n",
    "        v2.RandomChoice([\n",
    "            v2.ColorJitter(brightness=0.4, contrast=0.2, hue =0.3), #随机改变图像的亮度、对比度、饱和度和hue\n",
    "            v2.RandomRotation(degrees=(0, 30)),                     #随机对图像在给定范围内正负rotation\n",
    "            v2.RandomInvert(p=1.0),                                 #按指定概率对图像颜色进行反转\n",
    "            v2.RandomSolarize(192, p=1.0),                          #按阈值对图像颜色进行反转\n",
    "            v2.RandomAutocontrast(p=1.0),                           #调整对比度\n",
    "            v2.RandomAdjustSharpness(2.0, p=1.0),                   #锐化\n",
    "            v2.RandomHorizontalFlip(p=1.0),                         #水平翻转\n",
    "        ])\n",
    "    ], p=0.35),\n",
    "    v2.ToDtype(torch.float32, scale=True)                           #scale是否将其值缩放到[0,1]之间即标准化\n",
    "])\n",
    "\n",
    "#通常我们不需要任何在val阶段和test阶段的augmentation，只需要resize再转化为tensor即可\n",
    "#但我们可以使用train_tfm来处理测试集来产生不同类型的图片，但使用集成方法来决定test的结果\n",
    "test_tfm = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(128, 128),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "#无需将测试数据转换为张量，如果这么做了就要在测试函数中还原，比较麻烦\n",
    "aug_tfm = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(128, 128)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c27cb-5ff3-4bea-ba5d-cd0029cc2965",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "对于数据在模型输入之前的处理，有两种结构，第一种是将数据处理放在构造函数中，这样再getitem中便不用再做处理  \n",
    "第二种则是定义好处理的流水线，例如transform层，在getitem中使用数据时再将数据进行变换。\n",
    "<span style=\"font-size: 16px; letter-spacing: normal;\">This is normal text.</span>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98656536-0ba3-400c-ac2e-c40fa4cb12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, mode, path, tfm=test_tfm, files=None):\n",
    "        super(ImageDataset).__init__()\n",
    "        self.transform = tfm\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'train' or mode == 'val':\n",
    "            '''\n",
    "            files:所有文件名的列表\n",
    "            train_path:原先划分好的训练数据path，待融合\n",
    "            val_path:原先划分好的val数据path，待融合\n",
    "            分别提取出train文件夹中的数据和validation文件夹中的文件名list，并merge为file便于交叉验证和其他处理\n",
    "            由于我们既然将两个文件夹中的图片融合了，那么他们之间就是等价的。无论怎么拼接都是可行的\n",
    "            '''\n",
    "            train_path, val_path = os.path.join(path,'training'), os.path.join(path,'validation')\n",
    "            files_train = sorted([str(Path(path) / 'training' / x) for x in os.listdir(train_path) if x.endswith(\".jpg\")])\n",
    "            files_val = sorted([str(Path(path) / 'validation' / x) for x in os.listdir(val_path) if x.endswith(\".jpg\")])\n",
    "            self.files = files_train + files_val\n",
    "            #在windows系统下进行文件路径等处理的时候必须进行一些打印输出的操作，否则很容易被不同的文件路径格式\n",
    "            print(self.files[:5])\n",
    "        else:\n",
    "            test_path = os.path.join(path,'test')\n",
    "            self.files = sorted([str(Path(path) / 'test' / x) for x in os.listdir(test_path) if x.endswith('.jpg')])\n",
    "        if files != None :\n",
    "                self.files = files\n",
    "        print(f\"first of Image samples\",self.files[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        除了用transform对图像进行处理之外，还需要提取图像名中的标签。为文件名的开头第一个数字\n",
    "        结构 Folder1/Floder2/0_1234.jpg 在这个路径中生成以 / 为分隔符的列表，再取列表最后一个\n",
    "        元素，即'0_1234.jpg'进行处理,同理再进行一次 _ 分割就可以得到标签。\n",
    "\n",
    "        即使测试集用不到标签，我们也对其处理以下\n",
    "        '''\n",
    "        fname = self.files[index]\n",
    "        image = Image.open(fname)\n",
    "        if(image == None): print('None image while getitem fname:',fname)\n",
    "            \n",
    "        if self.mode == 'train' or self.mode == 'val': \n",
    "            image = self.transform(image)\n",
    "            try:\n",
    "                label = int(fname.split('\\\\')[-1].split('_')[0])\n",
    "                assert label >= 0 and label <= 10 ,\"Labels are out of bounds\"\n",
    "            except:\n",
    "                label = -1\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "            label = -1\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "    def generate_augmentation_testfilelist(self, fname):\n",
    "        '''\n",
    "        fname:测试图片的文件名/路径\n",
    "        生成测试图片的所有增强并返回包含原图片的所有增强的图片列表\n",
    "        return:imagelist:从原图片开始按顺序排列的增强图片列表\n",
    "        '''\n",
    "        imagelist = []\n",
    "        original = Image.open(fname)\n",
    "        original = v2.ToImage(original)\n",
    "        original = F.resize(original, 128, 128)\n",
    "        imagelist.append(orignal)\n",
    "        \n",
    "        v2.ToDtype(torch.float32, scale=True)\n",
    "        picture = F.to_dtype(picure, torch.float32, scale=True)\n",
    "        \n",
    "        augmentations = [\n",
    "            v2.ColorJitter(brightness=0.4, contrast=0.2, hue =0.3), #随机改变图像的亮度、对比度、饱和度和hue\n",
    "            v2.RandomRotation(degrees=(0, 30)),                     #随机对图像在给定范围内正负rotation\n",
    "            v2.RandomInvert(p=1.0),                                 #按指定概率对图像颜色进行反转\n",
    "            v2.RandomSolarize(192, p=1.0),                          #按阈值对图像颜色进行反转\n",
    "            v2.RandomAutocontrast(p=1.0),                           #调整对比度\n",
    "            v2.RandomAdjustSharpness(2.0, p=1.0),                   #锐化\n",
    "            v2.RandomHorizontalFlip(p=1.0),                         #水平翻转\n",
    "        ]\n",
    "        for i, transform in enumerate(augmentations):\n",
    "            transformed_image = transform(original)\n",
    "            transformed_image = F.to_dtype(transformed_image, torch.float32, scale = True)\n",
    "            imagelist.append(transformed_image)\n",
    "        return imagelist \n",
    "\n",
    "    def get_files(self):\n",
    "        '''\n",
    "        返回所有文件路径列表使得KFold可根据文件名划分\n",
    "        '''\n",
    "        return self.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e08342-8e81-4399-b382-c0ec474f5a83",
   "metadata": {},
   "source": [
    "## Model\n",
    "由于计算量太大，所以要求实现的网络比较浅而且并不是每一个连接处都有shortcut，而是隔一个才有一个\n",
    "\n",
    "由于resnet论文中并没有详细说明实现，只能看源码来学习数据流动了。经过查看源码对于原文中较浅的resnet来说\n",
    "未使用bottleNeck结构的结构如下：\n",
    "* 每次跳跃两层，并且identity与Residual总是在进入卷积层之前分离，在经过两层卷积层之后ReLU之前汇合，对于主路线有如下数据流动方式\n",
    "* Residual与identity分离 Residual进入->Conv->BatchNorm->ReLU->Conv->Batch-> Residual + identity ->ReLU\n",
    "从下方的源码和原论文中的模型结构可以看出只有中间四个模块化主要层是进行了分离处理的，开头7 * 7的卷积核处理、batchNorm\n",
    "ReLU和pool都是将残差和identity并不分离一起处理的，这一点在代码实现中需要注意。在经过四个主要卷积模块处理之后\n",
    "最后也是将数据都汇入主干进行进行平均池化和全连接层的处理，全连接层设置在softmax之前是必要的\n",
    "\n",
    "\n",
    "```python\n",
    "    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding\n",
    "        =1)\n",
    "        # 通过_make_layer带到层次化设，只有中间这四行中包含shortcut实现计的效果\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])  # 对应着conv2_x\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])  # 对应着conv3_x\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])  # 对应着conv4_x\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])  # 对应着conv5_x\n",
    "        # 分类头\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expan\n",
    "### 步幅为1的卷积和步幅为2的最大池化还是直接步幅为2的下采样？\n",
    "* 卷积 + 步长为2最大池化：\n",
    "在很多网络架构中，如 VGG，卷积层后跟最大池化是一种常见的设计。它可以在保持特征表达能力的同时减少计算量。\n",
    "这种设计可以使网络更具稳定性，因为最大池化减少了特征图的空间尺寸，并且有助于防止过拟合。\n",
    "\n",
    "* 步长为 2 的卷积(SownSampling)：\n",
    "在许多现代卷积神经网络（如 ResNet 和 DenseNet）中，步长为 2 的卷积是常见的下采样方法。这种方法可以更有效地融合特征，因为它在下采样的同时进行特征提取。\n",
    "使用步长为 2 的卷积可以提高网络的表达能力，因为卷积操作能够整合更多的信息，从而可以表现得更好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec0bf4-2dcf-482b-9f56-1813ba0e3da0",
   "metadata": {},
   "source": [
    "<img src = 'structure.png' alt = 'structure.png' width = '600' position = 'center'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e27fc9-2808-41a5-a846-7990f86e0ec8",
   "metadata": {},
   "source": [
    "### BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe5f67d-83bd-47f7-842e-4202b3954768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size(square), stride, padding)\n",
    "# torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self,name):\n",
    "        super(PrintLayer,self).__init__()\n",
    "        self.name = name\n",
    "    def forward(self, x):\n",
    "        print(f'after {self.name} x.shape = {x.shape}')\n",
    "        return x\n",
    "        \n",
    "class BasicBlock(nn.Module):\n",
    "    # expansion: int = 1\n",
    "    '''\n",
    "    基础模块以及identity shortcut的实现\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 inplanes: int,\n",
    "                 midplanes: int,\n",
    "                 outplanes: int):\n",
    "        \n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        #self.cnn = nn.Sequential(\n",
    "        #由于在实现Dataset的数据增强或是只是做transform处理的部分已经进行了标准化，所以在模型的初始阶段无需进行标准化\n",
    "        self.conv1 = nn.Conv2d(inplanes, midplanes, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(midplanes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)#can optionally do the operation in-place. Default: False\n",
    "        # self.pool1 = nn.MaxPool2d(3, 2, 0) 抛弃最大池化而采用下采样来降低维度\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(midplanes, outplanes, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(outplanes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # self.pool2 = nn.MaxPool2d(3, 2, 0)\n",
    "\n",
    "        #下采样 用于处理shortcut传递identity与主线路大小不匹配的问题，下采样之后也是需要标准化处理的\n",
    "        self.downSample = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, midplanes, kernel_size=1, stride=2,padding=0),\n",
    "            # nn.Conv(midplanes, outplanes, kernel_size=1, stride=2)\n",
    "            nn.BatchNorm2d(midplanes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x is None:\n",
    "            raise ValueError(\"Input is None in BasicBlock forward method\")\n",
    "       \n",
    "        # print(f\"Input shape to BasicBlock: {x.shape}\")#运行到这里就会出现 x 变成None的情况，但是在\n",
    "       \n",
    "        identity = x.clone()                          #残差和恒等映射分离\n",
    "        residual = x\n",
    "        residual = self.conv1(residual)\n",
    "        residual = self.bn1(residual)\n",
    "        # print(f'redsdiual shape after conv1 and bn1 :{residual.shape}')\n",
    "        \n",
    "        identity = self.downSample(identity)    #下采样以使得identity特征图的长宽和主线路上的长宽相同\n",
    "        # print(f\"identiy Shape after downSample: {identity.shape}\")\n",
    "        \n",
    "        x = residual + identity                  #根据要求给出的网络结构残差只跳过一层卷积层就回到主线路\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        # print(f\"x Shape after conv2 and bn2: {x.shape}\")\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cf3af-db6a-4d5a-9117-1b4e4084ae42",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c494f2cc-eaf8-427a-9ce0-6916bbcab73a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # torch.nn.Conv2d(in_channels, out_channels, kernel_size(square), stride, padding)\n",
    "# torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "class Classifier(nn.Module):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        #做类似ResidualNet原论文中初始化时候的一个处理，最然我也不知道为什么，今天就这样吧 23：56/8-1\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=3, bias=False), #64*64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),                #32*32\n",
    "        )\n",
    "        self.cnn = nn.Sequential(\n",
    "            BasicBlock(64, 64, 128),                                         #16*16\n",
    "            # PrintLayer('1 block'),\n",
    "            \n",
    "            BasicBlock(128, 128, 128),                                       #8*8\n",
    "            # PrintLayer('2 block'),\n",
    "            \n",
    "            BasicBlock(128, 256, 256),                                       #4*4\n",
    "            # PrintLayer('3 block'),\n",
    "            \n",
    "            BasicBlock(256, 512, 512),                                       #2*2\n",
    "            # PrintLayer('last block'),\n",
    "        )\n",
    "        #平均池化 + \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),            #这一步很重要平均池化之后经过全连接层进行处理\n",
    "            nn.Flatten(start_dim=1,end_dim=-1),      #在平均池化处理之后需要进行展平处理\n",
    "            # PrintLayer('after Flatten size'),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11),\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def cal_loss(self, preds, labels):\n",
    "        return self.criterion(preds, labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed64fa-acf5-487a-936d-fd9dcf6dc299",
   "metadata": {},
   "source": [
    "## Train\n",
    "* 对张量除了做类型变换之外的操作得到的也是张量，所以每次需要在最后进行转换.item() .numpy()转化为数值类型或是数组类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b2feea-38eb-408e-8177-eaacb1318c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "def train(train_set: DataLoader, \n",
    "          val_set: DataLoader, \n",
    "          model: nn.Module, \n",
    "          config: Any, \n",
    "          device: torch.device) -> None:\n",
    "    '''\n",
    "        通常train都需要五个参数以上\n",
    "        train_set:training dataset(dataLoader)\n",
    "        val_set:validation dataset(dataLoader)\n",
    "        model:model instance\n",
    "        config:hyperParameter\n",
    "        device:compute device\n",
    "    '''\n",
    "    #-----------------------------training--------------------------------#\n",
    "    model.train()\n",
    "    optimizer = getattr(torch.optim,config['optimizer'])(\n",
    "        model.parameters(),**config['optimizer_hParas'])\n",
    "    n_epochs = config['n_epochs']\n",
    "    epoch = 0\n",
    "    best_acc = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        train_batch_loss = []\n",
    "        train_batch_accs = []\n",
    "        for x, y in train_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # if x == None :\n",
    "            #     print('Rasie NoneTypeError:X is None')\n",
    "            \n",
    "            preds = model(x)\n",
    "            \n",
    "            assert preds.size(-1) == 11,\\\n",
    "            \"Output dimension does not match the number of classes\"\n",
    "            # print(f'prediciton after epoch{epoch+1} {preds}')\n",
    "            # print(f'label value:{y}')\n",
    "            \n",
    "            loss = model.cal_loss(preds, y)\n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm = 10)\n",
    "            optimizer.step()\n",
    "\n",
    "            #Another method to calculate batchAcc,下面这种计算一个batch的acc的方式与HW2中的计算Acc的方法显然更好\n",
    "            #比较简洁并且不容易出错，不需要考虑len(x)而是指即取平均值\n",
    "            acc = (preds.argmax(dim=-1).detach() == y.detach()).float().mean()\n",
    "            train_batch_accs.append(acc)\n",
    "            \n",
    "            loss_detached = loss.detach().cpu().item()      #detach loss\n",
    "            train_batch_loss.append(loss_detached)\n",
    "            \n",
    "        train_loss = sum(train_batch_loss) / len(train_batch_loss)\n",
    "        train_acc = sum(train_batch_accs) / len(train_batch_accs)\n",
    "        print('[{:03d}/{:03d}] Train Acc：{:3.6f} Train Loss：{:3.6f}'.format(epoch+1, n_epochs, train_acc, train_loss))\n",
    "        \n",
    "        #-----------------------------validation--------------------------------#\n",
    "        if len(val_set) > 0:\n",
    "            val_acc, val_loss = validate(val_set, model, device)\n",
    "            print('Val Acc：{:3.6f} Val Loss：{:3.6f}'.format(val_acc, val_loss))\n",
    "    \n",
    "            if val_acc > best_acc :\n",
    "                best_acc = val_acc\n",
    "                 # only save best to prevent output memory exceed error\n",
    "\n",
    "                torch.save(model.state_dict(),config['save_path'])\n",
    "                print('Saving model at epoch {:03d} Better Acc {:3.3f}'.format(epoch + 1, val_acc))\n",
    "            \n",
    "            else:\n",
    "                print('Epoch {:03d} Not a better Acc {:3.3f}'.format(epoch + 1,val_acc))\n",
    "        elif len(val_set) == 0:\n",
    "            torch.save(model.state_dict(), config['save_path'])\n",
    "            print('Saving model at last epoch')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444e260-22dc-4bea-8bbd-a69799f33509",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af8bb6a-9982-4ba3-aa42-855e9e6e7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_set: DataLoader, model:nn.Module, device:torch.device):\n",
    "    model.eval()\n",
    "    val_epoch_acc = []\n",
    "    val_epoch_loss = []\n",
    "    for x, y in val_set:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(x)\n",
    "            loss = model.cal_loss(preds, y).detach().cpu().item()\n",
    "        acc = (preds.argmax(dim=-1).detach() == y.detach()).float().mean()\n",
    "        val_epoch_acc.append(acc)\n",
    "        val_epoch_loss.append(loss)\n",
    "    return sum(val_epoch_acc) / len(val_epoch_acc), sum(val_epoch_loss) / len(val_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851492bb-5531-4cd0-8a59-c4f67815f91f",
   "metadata": {},
   "source": [
    "## Test\n",
    "这里需要对测试数据进行增强后预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea976044-b1a8-4b97-8146-7be81b347410",
   "metadata": {},
   "source": [
    "### 一、不改动__getitem__()而在测试函数中定义变换并进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd21ff-ecc8-44cf-bf20-5000f05c7ba3",
   "metadata": {},
   "source": [
    "```python\n",
    "# 定义数据增强变换列表\n",
    "augmentations = [\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.RandomVerticalFlip(p=1.0),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "]\n",
    "\n",
    "# 定义基本变换（无需增强时）\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 初始化数据集和数据加载器\n",
    "test_dataset = ImageDataset('path/to/test/images', transform=basic_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 测试函数，思想在于不是一次性生成所有增强图片而是一次处理一个，这里需要了解DataLoader的工作原理\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            original_outputs = model(images)\n",
    "            augmented_outputs = []\n",
    "            \n",
    "            for aug in augmentations:\n",
    "                augmented_transform = transforms.Compose([aug, transforms.ToTensor()])\n",
    "                #下面对图像处理的操作是已经经过getitem的变换\n",
    "                augmented_image = aug(images.cpu().squeeze().permute(1, 2, 0))\n",
    "                augmented_image = augmented_transform(augmented_image)\n",
    "                augmented_image = augmented_image.unsqueeze(0).to(device)\n",
    "                output = model(augmented_image)\n",
    "                augmented_outputs.append(output)\n",
    "            \n",
    "            # Combine original and augmented outputs\n",
    "            #可以自行改变加权方式\n",
    "            all_outputs = torch.stack([original_outputs] + augmented_outputs)\n",
    "            combined_output = torch.mean(all_outputs, dim=0)\n",
    "            results.append(combined_output.cpu().numpy())\n",
    "    \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8934a-b7ef-43c8-89cb-f44b1995ce4d",
   "metadata": {},
   "source": [
    "### 二、在__getitem__进行复杂预处理是不推荐的，会占用过多内存\n",
    "DataLoader的内存占用情况与BatchSize和实现的Dataset中__init__()/__ getitem__()行为密切相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112bf2f6-add9-4c86-9cea-e9380d8774ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_set: DataLoader, \n",
    "         model: nn.Module, \n",
    "         device: torch.device, \n",
    "         weight: list):\n",
    "    '''\n",
    "        测试函数，使用训练好的模型对需要预测的数据进行预测\n",
    "        这里的test实现需要用到通过对增强过的数据加权来进行预测,通过Dataset的getitem来返回\n",
    "        包含原图片的所有增强图片的列表，再\n",
    "    '''\n",
    "    model.eval()\n",
    "    augmentation = [\n",
    "        v2.ColorJitter(brightness=0.4, contrast=0.2, hue=0.3),\n",
    "        v2.RandomRotation(degress=(0, 30)),\n",
    "        v2.RandomInvert(p=1.0),\n",
    "        v2.RandomSolarize(192, p=1.0),\n",
    "        v2.RandomAutocontrast(p=1.0),\n",
    "        v2.RandomAdjustSharpness(2.0, p=1.0),\n",
    "        v2.RandomHorizontalFlip(p=1.0)\n",
    "    ]\n",
    "\n",
    "    basic_tfm = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "    prediction = []                                   #存储一张图片的分类结果 \n",
    "    preds = []                                        #存储所有图片的分类结果\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            prediction.append(model(x).detach().cpu())              #原始图片的预测结果\n",
    "            \n",
    "            for i, aug in enumerate(augmentation):                  #枚举每一个变换\n",
    "                transformed = aug(x)                                #增强变换\n",
    "                transformed = basic_tfm(x)                          #基础变换，转换为张量\n",
    "                prediction.append(model(x).detach().cpu())          #所有预测结果都存储在一个张量列表中\n",
    "            \n",
    "            weight = torch.tensor(weight)                           #权重列表\n",
    "            weight_sum = sum(w*p for w, p in zip(weight,prediction))#通过列表表达式对每个图片的预测概率进行加权加和\n",
    "           \n",
    "            _,maxProbClass = tensor.max(weight,dim=0)               #找出最大概率对应的标签\n",
    "            preds.append(maxProbClass)                              \n",
    "        preds = torch.cat(preds,dim=0).numpy()                      #将概率对应标签进行拼接并转换为numpy类型\n",
    "        \n",
    "    return preds\n",
    "\n",
    "def save_preds(preds, file):\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w') as fp:\n",
    "        writer.writerow(['Id', 'Class'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab9844-7842-4bf6-bdcb-cc7a3da2a862",
   "metadata": {},
   "source": [
    "## TrainingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3757037-305f-42ea-9c83-c77bc3f95207",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "config = {\n",
    "    'n_epochs':10,\n",
    "    'batch_size':128,\n",
    "    'optimizer':'Adam',\n",
    "    'optimizer_hParas':{\n",
    "        'lr':1e-3,\n",
    "        # 'monmentum':0.9,\n",
    "        # 'weight_decay':5e-4\n",
    "    },\n",
    "    'save_path':'models/model.pth',\n",
    "    'n_folds':10,\n",
    "    'weight':[0.5, 0.5/7, 0.5/7, 0.5/7, 0.5/7, 0.5/7, 0.5/7, 0.5/7]\n",
    "}\n",
    "path = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d9b27-ddab-450e-b688-7e5a1a956075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset\\\\training\\\\0_0.jpg', 'dataset\\\\training\\\\0_1.jpg', 'dataset\\\\training\\\\0_10.jpg', 'dataset\\\\training\\\\0_100.jpg', 'dataset\\\\training\\\\0_101.jpg']\n",
      "first of Image samples dataset\\training\\0_0.jpg\n",
      "13296\n",
      "FOLD 1\n",
      "---------------------------------------------------\n",
      "[001/010] Train Acc：0.301143 Train Loss：1.996295\n",
      "Val Acc：0.258551 Val Loss：2.389072\n",
      "Saving model at epoch 001 Better Acc 0.259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# 加载数据集\n",
    "dataset = ImageDataset('train', path=path, tfm=train_tfm)\n",
    "print(len(dataset))\n",
    "\n",
    "# 初始化 KFold\n",
    "kfold = KFold(n_splits=config['n_folds'], shuffle=True, random_state=myseed)\n",
    "\n",
    "# 开始 K-fold 训练\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(dataset)))):\n",
    "    print(f'FOLD {fold+1}')\n",
    "    print('---------------------------------------------------')\n",
    "    \n",
    "    # 每个折叠都重新初始化模型\n",
    "    model = Classifier().to(device)\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    trainloader = DataLoader(dataset, batch_size=config['batch_size'], sampler=train_sampler)\n",
    "    valloader = DataLoader(dataset, batch_size=config['batch_size'], sampler=val_sampler)\n",
    "    \n",
    "    # 开始训练\n",
    "    train(train_set=trainloader, val_set=valloader, model=model, config=config, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a0f44-54a6-4b63-95f5-23538996c852",
   "metadata": {},
   "source": [
    "## TestingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9dda3-4393-4fea-86f6-7f8e62c7ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_best = Classifier().to(device)\n",
    "# model_best.load_state_dict(torch.load(config['save_path']))\n",
    "# test_dataset = ImageDataset('test', path=path, tfm=aug_tfm)\n",
    "\n",
    "# preds = test(test_dataset, models_best, device, config['weight'])\n",
    "# save_preds(preds,'pred.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be713cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "#dataPreprocess\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#plottingTools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "#set a random seed for generation of random number by CPU/NUMPY/GPU\n",
    "myseed = 420613\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9807a6",
   "metadata": {},
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de98d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    '''geting device if cuda is available'''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def polt_learning_curve(loss_record,title = ''):\n",
    "    '''plot learning curve of your DNN()(train&dev loss)'''\n",
    "    total_steps = len(loss_record[train])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x1[::len(loss_record['train'])//len(loss_record['dev'])]\n",
    "    figure(figsize=(6,4))\n",
    "    plt.plot(x_1,loss_record['train'],c = 'tab:red', label = 'train')\n",
    "    plt.plot(x_2,loss_record['dev'],c = 'tab:cyan', label = 'dev')\n",
    "    plt.ylim(0.0,5.)\n",
    "    plt.xlabel('TrainSteps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pred(dv_set, model, device, lim = 35., preds = None, targets = None):\n",
    "    '''plot predition of your DNN'''\n",
    "    if preds is None or targets is None:\n",
    "        model.eval()#set model to eval mode\n",
    "        preds, targets = [], []\n",
    "        for x, y in dv_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                preds.append(pred.detach().cpu())\n",
    "                targets.append(y.detach().cpu())\n",
    "        preds = torch.cat(preds, dim = 0).numpy()\n",
    "        targets = torch.cat(target, dim = 0).numpy()\n",
    "        \n",
    "    figure(figsize=(5,5))\n",
    "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
    "    plt.plot([-0.2,lim], [-0.2, lim], c = 'b')\n",
    "    plt.xlim(-0.2,lim)\n",
    "    plt.ylim(-0.2,lim)\n",
    "    plt.xlabel('ground truth value')\n",
    "    plt.ylabel('predicted value')\n",
    "    plt.title('Ground Truth with Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c6df6",
   "metadata": {},
   "source": [
    "# **Preprocess**\n",
    "We have three kinds of dataset:\n",
    "* `train`:for training\n",
    "* `dev`:for validation\n",
    "* `test`:for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224cbdc",
   "metadata": {},
   "source": [
    "# **Dataset**\n",
    "The covid2019Dataset below dose:\n",
    "*  read `csv` files\n",
    "*  extract features\n",
    "*  spilt `covid.train.csv` dataset into train/dev sets\n",
    "*  normalize features\n",
    "\n",
    "Finishing TODO might make you pass baseline.\n",
    "\n",
    "**数据第0行为列名从第一行开始；第0列为ID，1到93列为特征，第94列即最后一列为标签，即positivetested_positive\n",
    "    ![image.png](datasetStructure.png)**\n",
    "# Feature extract\n",
    "**除非数据特征特别少，否则都要进行特征选择，筛选与目标值相关性强的属性来作为神经网络的输入下面为特征选择阶段，通过f_regerssion评价特征与目标值之间的相关度得分来选择是否将某个特征作为神经网络的输入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00066314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Specs          Score\n",
      "75   tested_positive.1  148069.658278\n",
      "57     tested_positive   69603.872591\n",
      "42        hh_cmnty_cli    9235.492094\n",
      "60      hh_cmnty_cli.1    9209.019558\n",
      "78      hh_cmnty_cli.2    9097.375172\n",
      "43      nohh_cmnty_cli    8395.421300\n",
      "61    nohh_cmnty_cli.1    8343.255927\n",
      "79    nohh_cmnty_cli.2    8208.176435\n",
      "40                 cli    6388.906849\n",
      "58               cli.1    6374.548000\n",
      "76               cli.2    6250.008702\n",
      "41                 ili    5998.922880\n",
      "59               ili.1    5937.588576\n",
      "77               ili.2    5796.947672\n",
      "92  worried_finances.2     833.613191\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('covid.train.csv')\n",
    "x = data[data.columns[1:94]]\n",
    "y = data[data.columns[94]]\n",
    "\n",
    "# print(data.columns[1:94])\n",
    "\n",
    "# print(y)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = (x - x.min())/(x.max() - x.min())\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k = 5)\n",
    "fit = bestfeatures.fit(x,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "# print(dfscores)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "# print(dfcolumns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']\n",
    "print(featureScores.nlargest(15,'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548341e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## 数据处理经验总结\n",
    "* `1.`使用list来存储特征列index，使用list来存储split的行号\n",
    "* `2.`在选择行和列结束之后对self.data进行赋值时候numpy转化成tensor\n",
    "* `3.`对数据的标准化处理等修改数据的行为都直接对self.data进行\n",
    "* `4.`不要在data转换为self.data之前对data进行修改\n",
    "'''\n",
    "class COVID2019Dataset(Dataset):\n",
    "    '''dataset for loading and preprocessing the Covid2019Dataset'''\n",
    "    def __init__(self,\n",
    "             path,\n",
    "             mode = 'train',\n",
    "             target_only = 'False'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        #read the data into numpy arrays\n",
    "        #使用with关键字使得异常处理自动进行，with经常在处理文件时使用,并且其会自动关闭文件\n",
    "        with open(path,'r') as fp:\n",
    "            data = list(csv.reader(fp))\n",
    "            data = np.array(data[1:])[:,1:].astype(float)\n",
    "        \n",
    "        #如果不是只选择提取出来的特征，那么将所有特征作为输入。如果只需要目标特征\n",
    "        #那么就只选择相关度高的特征\n",
    "        if not target_only:\n",
    "            feats = list(range(93))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            feats = [75,57,42,60,78,43,61,79,40,5,76,41,59,77,92]\n",
    "        \n",
    "        if mode == 'test':\n",
    "            #testing data:894 * 93\n",
    "            #93 = 40 states + 18(17+1)(day1) + 18(17+1))(day2) + 17(day3) \n",
    "            data = data[:,feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            #train data:2700 * 94\n",
    "            #94 = 40 + 18 + 18 + 18\n",
    "            target = data[:,-1]\n",
    "            data = data[:feats]\n",
    "            \n",
    "            #splitting data into train and dev dataset\n",
    "            '''\n",
    "            len(data) 用于获取二维数组的行数。\n",
    "            len(data[0]) 用于获取二维数组的列数（假设每行都有相同数量的元素）,\n",
    "            即获取data[0]中的元素个数而不是data[0]的个数,首先获取对应行数据的index\n",
    "            这样也便于后续标准化的处理，将numpy转化为张量之后再进行标准化即可\n",
    "            '''\n",
    "            if mode == 'dev':\n",
    "                indice = [i for i in range(len(data)) if i%10 == 0]\n",
    "            elif mode == 'train':\n",
    "                indice = [i for i in range(len(data)) if i%10 != 0]\n",
    "            \n",
    "            self.data = torch.FloatTensor(data[indice])\n",
    "            self.target = torch.FloatTensor(target[indice])\n",
    "            \n",
    "        #Normalize features，try to remove this part to see what happen\n",
    "        #官网解释dim参数：dim (int) – the dimension to reduce（要压缩的那个参数）\n",
    "        self.data = self.data[:, 40:] - \\\n",
    "        self.data[:, 40 :].mean(dim = 0, keepdim = True)\\\n",
    "        /self.data[:, 40:].std(dim = 0, keepdim = True)\n",
    "        \n",
    "        self.dim = shape(self.data[1])\n",
    "        \n",
    "        print('Finishing reading the {} set of Covid2019Dataset including\\ \n",
    "              {} samples each sample with dim = {}',format(mode,len(self.data),self.dim))\n",
    "              \n",
    "    def __getitem__(self,index):\n",
    "        if self.mode in ['train','dev']:\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "              \n",
    "                 \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2ea9c",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "A `DataLoader` loads data from a given Dataset into batches.\n",
    "* dataset (Dataset) – 加载数据的数据集。\n",
    "* batch_size (int, optional) – 每个batch加载多少个样本(默认: 1)。\n",
    "* shuffle (bool, optional) – 设置为True时会在每个epoch重新打乱数据(默认: False).\n",
    "* sampler (Sampler, optional) – 定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。\n",
    "* num_workers (int, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)\n",
    "* collate_fn (callable, optional) –是一个可调用对象，用于定义如何将一个 batch 的样本组合在一   起。通常，DataLoader 会自动将多个样本堆叠成一个批次（batch）。但在某些情况下，你可能需要自定   义这个过程，例如处理变长序列、不同形状的数据、或者执行一些特殊的预处理操作。\n",
    "* pin_memory (bool, optional) – pin_memory：将数据放置在锁页内存中，以加速从 CPU 到 GPU 的   数据传输，适用于 GPU 训练时提高数据加载效率。\n",
    "* drop_last (bool, optional) – 如果数据集大小不能被batch size整除，则设置为True后可删除最后   一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更*   小。(默认: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf63eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#官网解释：数据加载器。组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。\n",
    "def prepDataLoader(path, mode, batch_size, n_jobs = 0, target_only = False):\n",
    "    '''Generate dataset and put it into a dataloader.'''\n",
    "    #使用参数初始化实现的抽象类后续需要dataset来传给Dataloader进行迭代\n",
    "    dataset = COVID19Dataset(path,mode = mode,target_only = target_only)\n",
    "    dataloader = DataLoader(dataset, batch_size, \n",
    "                            shuffle=(mode == 'train'), \n",
    "                            drop_last=False,num_workers=n_jobs,\n",
    "                            pin_memory= True\n",
    "                           )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217b07b",
   "metadata": {},
   "source": [
    "# **Deep Neural Network**\n",
    "\n",
    "`NeuralNet` is an `nn.Module` designed for regression.\n",
    "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
    "This module also included a function `cal_loss` for calculating loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5397526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __int__(self,iput_dim):\n",
    "        super(NeuralNet,self).__int__()\n",
    "        #定义神经网络，更改模型使其表现得更佳\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    \n",
    "        #Mean square error loss\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "    \n",
    "    def cal_loss(self, pred, target, l2_lambda=0.0):\n",
    "        '''Calculate loss'''\n",
    "        # TODO: you may implement L1/L2 regulization here\n",
    "         #计算L2正则化项\n",
    "        mse_loss = self.criterion(pred,target)\n",
    "        l2_reg = 0.0\n",
    "        if l2_lambda > 0.0:\n",
    "            for parm in self.parameters():\n",
    "                l2_reg += torch.sum(parm ** 2)\n",
    "        return mse_loss + l2_lambda * l2_reg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381effdd",
   "metadata": {},
   "source": [
    "# Train/Dev/Test\n",
    "\n",
    "## Train\n",
    "**训练过程的实现主要是实现训练函数，训练函数的实现都是有步骤的，首先保证参数传递上的合理性，包含**\n",
    "* `训练集`\n",
    "* `验证集`\n",
    "* `模型`\n",
    "* `超参数配置`\n",
    "* `设备选择`\n",
    "\n",
    "**实现步骤 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(tr_set, dev_set, model, config, device):\n",
    "    \n",
    "    #setup optimizer\n",
    "    optimizer = getattr(torch.optim,config['optimizer'])\n",
    "    (model.parameters(), **config['optim_hparas'])\n",
    "    \n",
    "    n_epoch = config['n_epoch']\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train':[],'dev':[]}\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    while(epoch < n_epoch):\n",
    "        model.train()\n",
    "        for x,y = x.to(device),y.to(device):\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

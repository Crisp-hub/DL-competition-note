{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fdbaee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_path = 'covid.train.csv'  # path to training data\n",
    "tt_path = 'covid.test.csv'   # path to testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "be713cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "#dataPreprocess\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#plottingTools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "#set a random seed for generation of random number by CPU/NUMPY/GPU\n",
    "myseed = 420613\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9807a6",
   "metadata": {},
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5de98d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    '''geting device if cuda is available'''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def plot_learning_curve(loss_record,title = ''):\n",
    "    '''plot learning curve of your DNN()(train&dev loss)'''\n",
    "    total_steps = len(loss_record[train])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x1[::len(loss_record['train'])//len(loss_record['dev'])]\n",
    "    figure(figsize=(6,4))\n",
    "    plt.plot(x_1,loss_record['train'],c = 'tab:red', label = 'train')\n",
    "    plt.plot(x_2,loss_record['dev'],c = 'tab:cyan', label = 'dev')\n",
    "    plt.ylim(0.0,5.)\n",
    "    plt.xlabel('TrainSteps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pred(dv_set, model, device, lim = 35., preds = None, targets = None):\n",
    "    '''plot predition of your DNN'''\n",
    "    if preds is None or targets is None:\n",
    "        model.eval()#set model to eval mode\n",
    "        preds, targets = [], []\n",
    "        for x, y in dv_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                preds.append(pred.detach().cpu())\n",
    "                targets.append(y.detach().cpu())\n",
    "        preds = torch.cat(preds, dim = 0).numpy()\n",
    "        targets = torch.cat(target, dim = 0).numpy()\n",
    "        \n",
    "    figure(figsize=(5,5))\n",
    "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
    "    plt.plot([-0.2,lim], [-0.2, lim], c = 'b')\n",
    "    plt.xlim(-0.2,lim)\n",
    "    plt.ylim(-0.2,lim)\n",
    "    plt.xlabel('ground truth value')\n",
    "    plt.ylabel('predicted value')\n",
    "    plt.title('Ground Truth with Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c6df6",
   "metadata": {},
   "source": [
    "# **Preprocess**\n",
    "We have three kinds of dataset:\n",
    "* `train`:for training\n",
    "* `dev`:for validation\n",
    "* `test`:for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224cbdc",
   "metadata": {},
   "source": [
    "# **Dataset**\n",
    "The covid2019Dataset below dose:\n",
    "*  read `csv` files\n",
    "*  extract features\n",
    "*  spilt `covid.train.csv` dataset into train/dev sets\n",
    "*  normalize features\n",
    "\n",
    "Finishing TODO might make you pass baseline.\n",
    "\n",
    "**数据第0行为列名从第一行开始；第0列为ID，1到93列为特征，第94列即最后一列为标签，即positivetested_positive\n",
    "    ![image.png](datasetStructure.png)**\n",
    "# Feature extract\n",
    "**除非数据特征特别少，否则都要进行特征选择，筛选与目标值相关性强的属性来作为神经网络的输入下面为特征选择阶段，通过f_regerssion评价特征与目标值之间的相关度得分来选择是否将某个特征作为神经网络的输入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "00066314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Specs          Score\n",
      "75   tested_positive.1  148069.658278\n",
      "57     tested_positive   69603.872591\n",
      "42        hh_cmnty_cli    9235.492094\n",
      "60      hh_cmnty_cli.1    9209.019558\n",
      "78      hh_cmnty_cli.2    9097.375172\n",
      "43      nohh_cmnty_cli    8395.421300\n",
      "61    nohh_cmnty_cli.1    8343.255927\n",
      "79    nohh_cmnty_cli.2    8208.176435\n",
      "40                 cli    6388.906849\n",
      "58               cli.1    6374.548000\n",
      "76               cli.2    6250.008702\n",
      "41                 ili    5998.922880\n",
      "59               ili.1    5937.588576\n",
      "77               ili.2    5796.947672\n",
      "92  worried_finances.2     833.613191\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('covid.train.csv')\n",
    "x = data[data.columns[1:94]]\n",
    "y = data[data.columns[94]]\n",
    "\n",
    "# print(data.columns[1:94])\n",
    "\n",
    "# print(y)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = (x - x.min())/(x.max() - x.min())\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k = 5)\n",
    "fit = bestfeatures.fit(x,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "# print(dfscores)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "# print(dfcolumns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']\n",
    "print(featureScores.nlargest(15,'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "548341e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## 数据处理经验总结\n",
    "* `1.`使用list来存储特征列index，使用list来存储split的行号\n",
    "* `2.`在选择行和列结束之后对self.data进行赋值时候numpy转化成tensor\n",
    "* `3.`对数据的标准化处理等修改数据的行为都直接对self.data进行\n",
    "* `4.`不要在data转换为self.data之前对data进行修改\n",
    "'''\n",
    "class COVID2019Dataset(Dataset):\n",
    "    '''dataset for loading and preprocessing the Covid2019Dataset'''\n",
    "    def __init__(self,\n",
    "             path,\n",
    "             mode = 'train',\n",
    "             target_only = 'False'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        #read the data into numpy arrays\n",
    "        #使用with关键字使得异常处理自动进行，with经常在处理文件时使用,并且其会自动关闭文件\n",
    "        with open(path,'r') as fp:\n",
    "            data = list(csv.reader(fp))\n",
    "            data = np.array(data[1:])[:,1:].astype(float)\n",
    "        \n",
    "        #如果不是只选择提取出来的特征，那么将所有特征作为输入。如果只需要目标特征\n",
    "        #那么就只选择相关度高的特征\n",
    "        if not target_only:\n",
    "            feats = list(range(93))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            feats = [75,57,42,60,78,43,61,79,40,5,76,41,59,77,92]\n",
    "        \n",
    "        if mode == 'test':\n",
    "            #testing data:894 * 93\n",
    "            #93 = 40 states + 18(17+1)(day1) + 18(17+1))(day2) + 17(day3) \n",
    "            data = data[:,feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            #train data:2700 * 94\n",
    "            #94 = 40 + 18 + 18 + 18\n",
    "            target = data[:,-1]\n",
    "            data = data[:,feats]\n",
    "            \n",
    "            #splitting data into train and dev dataset\n",
    "            '''\n",
    "            len(data) 用于获取二维数组的行数。\n",
    "            len(data[0]) 用于获取二维数组的列数（假设每行都有相同数量的元素）,\n",
    "            即获取data[0]中的元素个数而不是data[0]的个数,首先获取对应行数据的index\n",
    "            这样也便于后续标准化的处理，将numpy转化为张量之后再进行标准化即可\n",
    "            '''\n",
    "            if mode == 'dev':\n",
    "                indice = [i for i in range(len(data)) if i%10 == 0]\n",
    "            elif mode == 'train':\n",
    "                indice = [i for i in range(len(data)) if i%10 != 0]\n",
    "            \n",
    "            self.data = torch.FloatTensor(data[indice])\n",
    "            self.target = torch.FloatTensor(target[indice])\n",
    "            \n",
    "        #Normalize features，try to remove this part to see what happen\n",
    "        #官网解释dim参数：dim (int) – the dimension to reduce（要压缩的那个参数）\n",
    "        self.data[:, 40:] = (self.data[:, 40:] - \\\n",
    "        self.data[:, 40:].mean(dim = 0, keepdim = True))\\\n",
    "        /self.data[:, 40:].std(dim = 0, keepdim = True)\n",
    "        \n",
    "        self.dim = self.data.shape[1]\n",
    "        \n",
    "        print('Finishing reading the {} set of Covid2019Dataset including {} samples\\\n",
    "        each sample with dim = {}'.format(mode,len(self.data),self.dim))\n",
    "              \n",
    "    def __getitem__(self,index):\n",
    "        if self.mode in ['train','dev']:\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "              \n",
    "                 \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2ea9c",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "A `DataLoader` loads data from a given Dataset into batches.\n",
    "* dataset (Dataset) – 加载数据的数据集。\n",
    "* batch_size (int, optional) – 每个batch加载多少个样本(默认: 1)。\n",
    "* shuffle (bool, optional) – 设置为True时会在每个epoch重新打乱数据(默认: False).\n",
    "* sampler (Sampler, optional) – 定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。\n",
    "* num_workers (int, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)\n",
    "* collate_fn (callable, optional) –是一个可调用对象，用于定义如何将一个 batch 的样本组合在一   起。通常，DataLoader 会自动将多个样本堆叠成一个批次（batch）。但在某些情况下，你可能需要自定   义这个过程，例如处理变长序列、不同形状的数据、或者执行一些特殊的预处理操作。\n",
    "* pin_memory (bool, optional) – pin_memory：将数据放置在锁页内存中，以加速从 CPU 到 GPU 的   数据传输，适用于 GPU 训练时提高数据加载效率。\n",
    "* drop_last (bool, optional) – 如果数据集大小不能被batch size整除，则设置为True后可删除最后   一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更*   小。(默认: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3cf63eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#官网解释：数据加载器。组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。\n",
    "def prepDataLoader(path, mode, batch_size, n_jobs = 0, target_only = False):\n",
    "    '''Generate dataset and put it into a dataloader.'''\n",
    "    #使用参数初始化实现的抽象类后续需要dataset来传给Dataloader进行迭代\n",
    "    dataset = COVID2019Dataset(path,mode = mode,target_only = target_only)\n",
    "    dataloader = DataLoader(dataset, batch_size, \n",
    "                            shuffle=(mode == 'train'), \n",
    "                            drop_last=False,num_workers=n_jobs,\n",
    "                            pin_memory= True\n",
    "                           )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217b07b",
   "metadata": {},
   "source": [
    "# **Deep Neural Network**\n",
    "\n",
    "`NeuralNet` is an `nn.Module` designed for regression.\n",
    "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
    "This module also included a function `cal_loss` for calculating loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5397526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        #定义神经网络，更改模型使其表现得更佳\n",
    "        \n",
    "        '''nn.Sequential 一个时序容器，Modules会以参数传入的顺序被添加到容器中。\n",
    "        当然，也可以传入一个OrderDict'''\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    \n",
    "        #Mean square error loss\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''定义了每次执行的计算步骤，在所有的子类中都要重写这个函数'''\n",
    "        return self.net(x).squeeze(1)\n",
    "    \n",
    "    def cal_loss(self, pred, target, l2_lambda=0.0):\n",
    "        '''Calculate loss'''\n",
    "        # TODO: you may implement L1/L2 regulization here\n",
    "         #计算L2正则化项\n",
    "        mse_loss = self.criterion(pred,target)\n",
    "        l2_reg = 0.0\n",
    "        if l2_lambda > 0.0:\n",
    "            for parm in self.parameters():\n",
    "                l2_reg += torch.sum(parm ** 2)\n",
    "        return mse_loss + l2_lambda * l2_reg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381effdd",
   "metadata": {},
   "source": [
    "# Train/Dev/Test\n",
    "\n",
    "## Training \n",
    "**训练过程的实现主要是实现训练函数，训练函数的实现都是有步骤的，首先保证参数传递上的合理性，包含**\n",
    "* `训练集`\n",
    "* `验证集`\n",
    "* `模型`\n",
    "* `超参数配置`\n",
    "* `设备选择`\n",
    "\n",
    "**实现步骤 \n",
    "1.定义optimizer，使用getattr来定义optimizer更灵活  \n",
    "2.初始化opoch，early_stop_cnt等  \n",
    "3.完成batch主循环，提取训练集特征向量和目标值，zero_grad() optimizer，将训练集特征x传入模型（正向传播），计算模型的损失函数值反向传播计算梯度。optimizer.step()梯度下降。每一个opch完成时记录训练集上的损失函数值  \n",
    "4.在epoch循环最后计算dev_set在每个opch调整之后的损失函数值（由dev函数实现dev损失函数的计算）比较当前模型在验证集上的表现决定是否保存模型。最后再所有epoch完成之后返回最小的mse以及记录的训练集和验证集上的记录即可**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "205d6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tr_set, dev_set, model, config, device):\n",
    "    \n",
    "    #setup optimizer\n",
    "    optimizer = getattr(torch.optim,config['optimizer'])(\n",
    "        model.parameters(), **config['optim_hparas'])\n",
    "    \n",
    "    n_epochs = config['n_epochs']\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train':[],'dev':[]}      #记录训练损失\n",
    "    early_stop_cnt = 0                       #早停，如果连续early_stop次验证集的损失都\n",
    "    epoch = 0                                #没有比之前的更小了就停止进行调整\n",
    "    while(epoch < n_epochs):                  \n",
    "        model.train()\n",
    "        for x,y in tr_set:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            #参考优化器参数的写法将L2正则化项的参数也写到config里由config一起管理模型的超参数\n",
    "            mse_loss = model.cal_loss(pred, y,config['L2_lambda'])\n",
    "            mse_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
    "            \n",
    "        '''\n",
    "        1.每一个epoch训练完成之后使用，使用验证集测试模型的性能，这一点非常重要，\n",
    "        如果模型在验证集上差距过大,我们就需要及时调整模型或是超参数避免继续训练下去浪费时间\n",
    "        2.将train、dev、test分离成独立模块便于在训练时进行调整\n",
    "        '''\n",
    "        dev_mse = dev(dev_set, model, device)\n",
    "        loss_record['dev'].append(dev_mse)\n",
    "        \n",
    "        if(dev_mse < min_mse):\n",
    "            min_mse = dev_mse\n",
    "            early_stop_cnt = 0\n",
    "            torch.save(model.state_dict(), config['save_path'])#保存模型\n",
    "            print(\"Saving model in epoch = {:4d}, loss = {:4f}\"\n",
    "                  .format(epoch+1, dev_mse))\n",
    "        else: early_stop_cnt += 1\n",
    "        \n",
    "        epoch+=1\n",
    "        if(early_stop_cnt > config['early_stop']):\n",
    "            #stop training\n",
    "            break\n",
    "    \n",
    "    print('Finishing training after {} epochs'.format(epoch))\n",
    "    return min_mse,loss_record "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c91c87",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f7a12d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dev(dev_set, model, device):\n",
    "    '''\n",
    "    与训练集计算损失有所区别，由于定义criterion为MSELoss时候reduction = mean\n",
    "    为标准的MSE公式，是对于整个batch而言的。所以乘每个batch的大小即可\n",
    "    '''\n",
    "    model.eval()#set model to eval mode\n",
    "    batch_mse = 0\n",
    "    total_mse = 0\n",
    "    for x,y in dev_set:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            dev_pred = model(x)\n",
    "            batch_mse = model.cal_loss(dev_pred,y,config['L2_lambda'])\n",
    "        total_mse += batch_mse.detach().cpu().item() * len(x)\n",
    "    total_mse /= len(dev_set)\n",
    "    return total_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5fc76",
   "metadata": {},
   "source": [
    "# Testing\n",
    "**测试集需要将预测的结果保存为第一列为序号第二列为标签值的csv文件再上传到kaggle进行评测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4b5885f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_set, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for x in test_set:\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds,dim = 0).numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834818c",
   "metadata": {},
   "source": [
    "# Setup Hypey-parameters\n",
    "`config` contains hyper-parameters for training and the path to save your model\n",
    "**将超参数封装成一个字典有助于管理和调整所有超参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cae0f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(device)\n",
    "os.makedirs('models',exist_ok = True)\n",
    "target_only = True\n",
    "\n",
    "config = {\n",
    "    'n_epochs':10000,\n",
    "    'batch_size':200,\n",
    "    'optimizer':'Adam',\n",
    "#     'optimizer':'SGD',\n",
    "    'optim_hparas':{\n",
    "        'lr':0.001,\n",
    "#         'momentum':0.9\n",
    "        #'weight_decay': 5e-4,\n",
    "    },\n",
    "    'early_stop':500,\n",
    "    'L2_lambda':0.00075,\n",
    "    'save_path':'models/model.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "dd60767e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Crisp\\AppData\\Local\\Temp\\ipykernel_10064\\2774795066.py:61: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  /self.data[:, 40:].std(dim = 0, keepdim = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing reading the train set of Covid2019Dataset including 2430 samples        each sample with dim = 15\n",
      "Finishing reading the dev set of Covid2019Dataset including 270 samples        each sample with dim = 15\n",
      "Finishing reading the test set of Covid2019Dataset including 893 samples        each sample with dim = 15\n"
     ]
    }
   ],
   "source": [
    "train_set = prepDataLoader(tr_path,'train',config['batch_size'],target_only=target_only)\n",
    "dev_set = prepDataLoader(tr_path,'dev', config['batch_size'], target_only=target_only)\n",
    "tt_set = prepDataLoader(tt_path, 'test',config['batch_size'], target_only=target_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "35de23da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet(train_set.dataset.dim).to(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dd4675bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in epoch =   63, loss = 994.568334\n",
      "Saving model in epoch =   67, loss = 962.232494\n",
      "Saving model in epoch =   71, loss = 950.650294\n",
      "Saving model in epoch =   76, loss = 830.512102\n",
      "Saving model in epoch =   77, loss = 818.908567\n",
      "Saving model in epoch =   85, loss = 763.106878\n",
      "Saving model in epoch =   92, loss = 683.039504\n",
      "Saving model in epoch =   99, loss = 666.470015\n",
      "Saving model in epoch =  109, loss = 597.546155\n",
      "Saving model in epoch =  117, loss = 570.978521\n",
      "Saving model in epoch =  127, loss = 533.354092\n",
      "Saving model in epoch =  128, loss = 528.761903\n",
      "Saving model in epoch =  138, loss = 467.835008\n",
      "Saving model in epoch =  141, loss = 464.422007\n",
      "Saving model in epoch =  143, loss = 459.246137\n",
      "Saving model in epoch =  144, loss = 414.388757\n",
      "Saving model in epoch =  155, loss = 395.584274\n",
      "Saving model in epoch =  159, loss = 358.732122\n",
      "Saving model in epoch =  160, loss = 352.325963\n",
      "Saving model in epoch =  170, loss = 321.360341\n",
      "Saving model in epoch =  181, loss = 315.935063\n",
      "Saving model in epoch =  187, loss = 263.162242\n",
      "Saving model in epoch =  190, loss = 254.717849\n",
      "Saving model in epoch =  200, loss = 245.557917\n",
      "Saving model in epoch =  201, loss = 240.229033\n",
      "Saving model in epoch =  204, loss = 232.944530\n",
      "Saving model in epoch =  206, loss = 224.171366\n",
      "Saving model in epoch =  211, loss = 218.063512\n",
      "Saving model in epoch =  213, loss = 213.315718\n",
      "Saving model in epoch =  222, loss = 198.665743\n",
      "Saving model in epoch =  233, loss = 188.387629\n",
      "Saving model in epoch =  234, loss = 180.776915\n",
      "Saving model in epoch =  248, loss = 175.091836\n",
      "Saving model in epoch =  252, loss = 171.811619\n",
      "Saving model in epoch =  256, loss = 166.276842\n",
      "Saving model in epoch =  258, loss = 165.745047\n",
      "Saving model in epoch =  261, loss = 153.579964\n",
      "Saving model in epoch =  273, loss = 144.942548\n",
      "Saving model in epoch =  294, loss = 135.833803\n",
      "Saving model in epoch =  303, loss = 130.925115\n",
      "Saving model in epoch =  307, loss = 130.297299\n",
      "Saving model in epoch =  327, loss = 129.671581\n",
      "Saving model in epoch =  347, loss = 125.276819\n",
      "Saving model in epoch =  349, loss = 124.435567\n",
      "Saving model in epoch =  369, loss = 123.357968\n",
      "Saving model in epoch =  371, loss = 121.538714\n",
      "Saving model in epoch =  377, loss = 118.781783\n",
      "Saving model in epoch =  467, loss = 118.709351\n",
      "Saving model in epoch =  481, loss = 117.507858\n",
      "Saving model in epoch =  511, loss = 117.317261\n",
      "Saving model in epoch =  567, loss = 115.621349\n",
      "Saving model in epoch =  744, loss = 115.602373\n",
      "Saving model in epoch =  750, loss = 114.839477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_loss,model_loss_record \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[208], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(tr_set, dev_set, model, config, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#参考优化器参数的写法将L2正则化项的参数也写到config里由config一起管理模型的超参数\u001b[39;00m\n\u001b[0;32m     19\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcal_loss(pred, y,config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL2_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmse_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m loss_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(mse_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_loss,model_loss_record = train(train_set, dev_set, model, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3851db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(model_loss_record, title='deep model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
    "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
    "model.load_state_dict(ckpt)\n",
    "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "\n",
    "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
    "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
